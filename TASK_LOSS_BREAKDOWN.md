# ğŸ“Š Task Loss Breakdown - SkillBlender Training

PhÃ¢n tÃ­ch chi tiáº¿t loss/reward structure cá»§a cÃ¡c task HRL (khÃ´ng tÃ­nh low-level skills) khi train SkillBlender.

---

## ğŸ¯ Task Loss Formula

```
Total Loss = Clip( sum(reward_i * scale_i) , min=0 if only_positive_rewards=True )
           + termination_penalty (náº¿u episode káº¿t thÃºc sá»›m)
```

**Key Points:**
- `only_positive_rewards = True` â†’ Clip total reward â‰¥ 0 (trÃ¡nh early termination)
- Má»—i step tÃ­nh reward tá»« cÃ¡c thÃ nh pháº§n
- Loss Ä‘Æ°á»£c accumulate qua episode (max 8000 steps cho task)

---

## ğŸ“‹ Task-Specific Reward Scales

### 1ï¸âƒ£ **task_transfer** - Chuyá»ƒn Box

**Config**: `/h1_task_transfer/h1_task_transfer_config.py` (line 207-245)

**Active Rewards** (Scale â‰  0):

| Component | Scale | Formula | Role |
|---|---|---|---|
| **box_pos** â­ | 5.0 | `exp(-4 * error)` | Primary: Box â†’ goal |
| **wrist_box_distance** | 1.0 | `exp(-4 * error)` | Secondary: Tay gáº§n box |

**Total Reward per step**:
```
R = 5.0 * box_pos(error)      [0 to 5]
  + 1.0 * wrist_distance(err) [0 to 1]
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Max â‰ˆ 6.0 (if all perfect)
  
Clipped to [0, âˆ) (only_positive_rewards = True)
```

**Inactive Rewards** (commented out):
```
joint_pos, wrist_pos, feet_clearance, feet_contact_number,
feet_air_time, foot_slip, feet_distance, knee_distance,
tracking_lin_vel, tracking_ang_vel, default_joint_pos, 
upper_body_pos, orientation, base_height, base_acc,
vel_mismatch_exp, low_speed, track_vel_hard, torques,
dof_vel, dof_acc, collision, action_smoothness
```

**Episode Total** (8 seconds = 8000 steps):
- Optimal: 8000 Ã— 6.0 = 48,000
- Realistic: 8000 Ã— 3-4 = 24,000-32,000
- Poor: 8000 Ã— 0-1 = 0-8,000

---

### 2ï¸âƒ£ **task_lift** - NÃ¢ng Box

**Config**: `/h1_task_lift/h1_task_lift_config.py` (line 197-245)

**Active Rewards**:

| Component | Scale | Formula | Role |
|---|---|---|---|
| **box_pos** â­ | 5.0 | `exp(-4 * error)` | Primary: Box nÃ¢ng lÃªn |
| **wrist_box_distance** â­ | 5.0 | `exp(-4 * error)` | Co-primary: Tay gáº§n box |

**Total Reward per step**:
```
R = 5.0 * box_pos(error)        [0 to 5]
  + 5.0 * wrist_distance(error) [0 to 5]
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Max â‰ˆ 10.0 (if all perfect)
```

**Key Difference from task_transfer**:
- âœ… `wrist_box_distance = 5.0` (vs 1.0) â†’ Tay pháº£i gáº§n box hÆ¡n
- âœ… Box_pos tracks Z axis (height) khÃ´ng pháº£i XY
- âŒ KhÃ´ng track velocity (chá»‰ position)

**Episode Total** (8 seconds = 8000 steps):
- Optimal: 8000 Ã— 10.0 = 80,000
- Realistic: 8000 Ã— 5-7 = 40,000-56,000
- Poor: 8000 Ã— 0-2 = 0-16,000

---

### 3ï¸âƒ£ **task_reach** - Cáº¥p Tay

**Config**: `/h1_task_reach/h1_task_reach_config.py` (line 204-240)

**Active Rewards**:

| Component | Scale | Formula | Role |
|---|---|---|---|
| **wrist_pos** â­ | 5.0 | `exp(-4 * error)` | Primary: Tay â†’ goal |

**Total Reward per step**:
```
R = 5.0 * wrist_pos(error) [0 to 5]
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Max â‰ˆ 5.0
```

**Simplest Task**:
- âœ… Chá»‰ track wrist position (khÃ´ng tracking box)
- âœ… Dá»… nháº¥t Ä‘á»ƒ converge
- âœ… 24 giÃ¢y episode (vs 8s cho task khÃ¡c)

**Episode Total** (24 seconds = 24000 steps):
- Optimal: 24000 Ã— 5.0 = 120,000
- Realistic: 24000 Ã— 3-4 = 72,000-96,000
- Poor: 24000 Ã— 0-1 = 0-24,000

---

### 4ï¸âƒ£ **task_carry** - Cáº§m Box Äi

**Config**: `/h1_task_carry/h1_task_carry_config.py` (line 199-237)

**Active Rewards** (Based on code):

| Component | Scale | Formula | Role |
|---|---|---|---|
| **box_pos** â­ | 5.0 | `exp(-4 * error)` | Primary: Box â†’ goal XYZ |
| **wrist_box_distance** â­ | 5.0 | `exp(-4 * error)` | Co-primary: Tay cáº§m cháº·t |

**Total Reward per step**:
```
R = 5.0 * box_pos(error)        [0 to 5]
  + 5.0 * wrist_distance(error) [0 to 5]
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Max â‰ˆ 10.0
```

**Similar to task_lift** nhÆ°ng:
- âœ… Box_pos tracks XYZ (chuyá»ƒn Ä‘á»™ng + nÃ¢ng)
- âœ… Cáº§n maintain grip khi Ä‘i (wrist_distance = 5.0)
- âŒ CÃ³ thá»ƒ phá»©c táº¡p hÆ¡n (cáº§n walk + hold)

---

### Other Tasks (ChÆ°a hoÃ n thiá»‡n)

**task_button, task_box, task_ball, task_cabinet**:
- Config cÃ³ sáºµn nhÆ°ng `reward.scales` **toÃ n bá»™ comment out**
- KhÃ´ng cÃ³ active reward nÃ o â†’ **Loss = 0**
- Cáº§n implement reward functions vÃ  uncomment scales

---

## ğŸ“ˆ Reward Component Details

### Primary Reward: `box_pos` (Scale = 5.0)

**Function**:
```python
def _reward_box_pos(self):
    box_pos_diff = self.box_root_states[:, :3] - self.box_goal_pos
    box_pos_error = torch.mean(torch.abs(box_pos_diff), dim=1)  # Mean error across XYZ
    return torch.exp(-4 * box_pos_error), box_pos_error
```

**Behavior**:
- Error = 0.0m â†’ Reward = 1.0 (exp(0) = 1)
- Error = 0.1m â†’ Reward â‰ˆ 0.67 (exp(-0.4) â‰ˆ 0.67)
- Error = 0.2m â†’ Reward â‰ˆ 0.45 (exp(-0.8) â‰ˆ 0.45)
- Error = 0.5m â†’ Reward â‰ˆ 0.14 (exp(-2) â‰ˆ 0.14)
- Error â‰¥ 1.0m â†’ Reward â‰ˆ 0 (exp(-4) â‰ˆ 0)

**Effective Range**: 0-0.5m (reward â‰¥ 0.14)

---

### Secondary Reward: `wrist_box_distance` (Scale = 1.0 or 5.0)

**Function**:
```python
def _reward_wrist_box_distance(self):
    wrist_pos = self.rigid_state[:, self.wrist_indices, :3]  # [N, 2, 3] - 2 hands
    wrist_pos = wrist_pos.flatten()  # [N, 6]
    box_pos = self.box_root_states[:, :3]  # [N, 3]
    wrist_box_diff = torch.norm(wrist_pos - box_pos.unsqueeze(1))  # Distance
    return torch.exp(-4 * error), error
```

**Behavior**:
- Distance = 0.0m â†’ Reward = 1.0
- Distance = 0.05m â†’ Reward â‰ˆ 0.82
- Distance = 0.1m â†’ Reward â‰ˆ 0.67
- Distance = 0.2m â†’ Reward â‰ˆ 0.45

**Role**:
- task_transfer: `scale=1.0` â†’ "Nice-to-have" (tay gáº§n box tá»‘t)
- task_lift/carry: `scale=5.0` â†’ "Must-have" (tay pháº£i cáº§m cháº·t)

---

### Wrist Position Reward (Scale = 5.0)

**task_reach only**:
```python
def _reward_wrist_pos(self):
    wrist_pos_diff = wrist_pos - ref_wrist_pos  # [N, 2, 3]
    wrist_pos_error = torch.mean(torch.abs(wrist_pos_diff), dim=1)
    return torch.exp(-4 * wrist_pos_error)
```

**Direct tracking of wrist to goal position**

---

## ğŸ¯ Loss During Training

### Example: task_transfer Training

**Iteration 0** (random policy):
```
box_pos error â‰ˆ 1.0m     â†’ box_pos reward â‰ˆ 0 Ã— 5.0 = 0
wrist_distance â‰ˆ 1.0m    â†’ wrist reward â‰ˆ 0 Ã— 1.0 = 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step reward â‰ˆ 0
Episode total â‰ˆ 0
```

**Iteration 1000** (learning):
```
box_pos error â‰ˆ 0.3m     â†’ box_pos reward â‰ˆ 0.30 Ã— 5.0 = 1.5
wrist_distance â‰ˆ 0.2m    â†’ wrist reward â‰ˆ 0.45 Ã— 1.0 = 0.45
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step reward â‰ˆ 1.95
Episode total â‰ˆ 1.95 Ã— 8000 â‰ˆ 15,600
```

**Iteration 10000** (convergence):
```
box_pos error â‰ˆ 0.05m    â†’ box_pos reward â‰ˆ 0.82 Ã— 5.0 = 4.1
wrist_distance â‰ˆ 0.05m   â†’ wrist reward â‰ˆ 0.82 Ã— 1.0 = 0.82
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step reward â‰ˆ 4.92
Episode total â‰ˆ 4.92 Ã— 8000 â‰ˆ 39,360
```

---

## ğŸ“Š Comparison Table

| Task | Episode Length | Num Rewards | Max Scale | Expected Convergence | Difficulty |
|---|---|---|---|---|---|
| **task_reach** | 24s | 1 | 5.0 | Easiest | â­ |
| **task_transfer** | 8s | 2 | 5.0+1.0=6.0 | Easy | â­â­ |
| **task_lift** | 8s | 2 | 5.0+5.0=10.0 | Medium | â­â­â­ |
| **task_carry** | 8s | 2 | 5.0+5.0=10.0 | Hard | â­â­â­â­ |

---

## ğŸ”§ Tuning Loss

### Náº¿u loss quÃ¡ nhá» (khÃ´ng converge):

```python
# TÄƒng scale cá»§a primary reward
class scales:
    box_pos = 10.0  # (tá»« 5.0) â†’ Enforce box position tracking máº¡nh hÆ¡n
    wrist_box_distance = 2.0  # (tá»« 1.0 or 5.0) â†’ Enforce grip máº¡nh hÆ¡n
```

### Náº¿u loss quÃ¡ lá»›n (unstable):

```python
# Giáº£m scale
class scales:
    box_pos = 2.0  # (tá»« 5.0)
    wrist_box_distance = 0.5  # (tá»« 1.0)
```

### Náº¿u agent fail (ngÃ£):

```python
# ThÃªm stability rewards
class scales:
    box_pos = 5.0
    wrist_box_distance = 1.0
    orientation = 1.0  # Giá»¯ thÃ¢n tháº³ng
    base_height = 0.2  # Giá»¯ hÃ´ng á»Ÿ Ä‘á»™ cao
    default_joint_pos = 0.5  # Giá»¯ tá»« tháº¿ chuáº©n
```

---

## ğŸ¯ Key Insights

| Aspect | Finding |
|---|---|
| **Simplest Task** | task_reach (wrist_pos only, 5.0 scale) |
| **Most Constrained** | task_lift/carry (10.0 combined scale, hard to balance) |
| **Least Stable** | task_transfer (asymmetric scales 5.0+1.0) |
| **Best for Learning** | task_reach (dá»… xem progress) |
| **Best for Complex Skill** | task_carry (nhiá»u constraint = learning signal) |

---

## ğŸš€ Recommended Loss Config for Better Convergence

```python
# Current (aggressive):
class scales:
    box_pos = 5.0
    wrist_box_distance = 1.0  # vs 5.0

# Proposed (balanced):
class scales:
    # Primary objective
    box_pos = 5.0
    # Secondary objective (penalty weight)
    wrist_box_distance = 2.0  # Increase to 2.0 for more stability
    # Add stability rewards
    orientation = 0.5
    base_height = 0.1
```

Vá»›i config nÃ y:
- âœ… Max scale = 7.6 (moderate)
- âœ… Clear priority (box > wrist > stability)
- âœ… Learning signal rÃµ rÃ ng
- âœ… KhÃ´ng quÃ¡ aggressive â†’ stable convergence
