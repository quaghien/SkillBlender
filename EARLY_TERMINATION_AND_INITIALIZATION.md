# ğŸ”„ Early Termination & Agent Initialization

TÃ³m táº¯t cÆ¡ cháº¿ káº¿t thÃºc episode sá»›m vÃ  khá»Ÿi táº¡o robot á»Ÿ cÃ¡c task.

---

## ğŸ›‘ Early Termination Conditions

### Locomotion Tasks (Walking, Reaching, etc.)

**Episode káº¿t thÃºc sá»›m khi:**

```python
# 1. CONTACT VIOLATION - Cháº¡m bá»™ pháº­n khÃ´ng Ä‘Æ°á»£c phÃ©p
reset_buf = torch.any(
    torch.norm(contact_forces[:, termination_contact_indices, :], dim=-1) > 1.0,
    dim=1
)
# Cháº¡m pháº¡t: ['pelvis', 'torso', 'shoulder', 'elbow'] (máº·c Ä‘á»‹nh)
# Náº¿u cháº¡m â†’ Ä‘iá»ƒm bá»‹ trá»« â†’ cÃ³ thá»ƒ terminate

# 2. TIMEOUT - Háº¿t thá»i gian episode
time_out_buf = episode_length_buf > max_episode_length
# max_episode_length = episode_length_s / dt (e.g., 24s / 0.001s = 24000 steps)
# KhÃ´ng cÃ³ terminal penalty

# 3. Cuá»‘i cÃ¹ng:
reset_buf |= time_out_buf  # Contact violation hoáº·c timeout Ä‘á»u reset
```

**Episode Length theo Task**:

| Task | Duration | Max Steps | Note |
|---|---|---|---|
| h1_walking | 24s | 24000 | Walking tá»± do |
| h1_reaching | 24s | 24000 | - |
| h1_squatting | - | - | - |
| h1_stepping | - | - | - |
| h1_task_transfer | 8s | 8000 | Short task |
| h1_task_lift | 8s | 8000 | Short task |
| h1_task_carry | 8s | 8000 | Short task |
| h1_task_reach | 24s | 24000 | Full skill combo |
| h1_task_ball | - | - | Plus: ball reaches goal â†’ reset |

---

### Task-Specific Termination (task_ball)

```python
# Extra: Ball reaches goal zone
ball_pos = self.ball_root_states[:, :3]
goal_pos = self.goal_pos
ball_goal_dist = torch.norm(ball_pos - goal_pos, dim=1)

# Reset náº¿u bÃ³ng Ä‘áº¿n goal
reset_buf |= ball_goal_dist < self.cfg.commands.ranges.threshold
```

**â†’ Success condition!** (khÃ´ng pháº£i failure)

---

## ğŸ¤– Agent Initialization (Reset)

### Giai Äoáº¡n Reset

```
reset_idx() Ä‘Æ°á»£c gá»i â†’ 3 bÆ°á»›c:
    1. _reset_dofs() - Khá»›p
    2. _reset_root_states() - Vá»‹ trÃ­ & váº­n tá»‘c chÃ­nh
    3. _resample_commands() - Lá»‡nh má»¥c tiÃªu má»›i
    4. Reset buffers - Action history, etc.
```

---

### 1ï¸âƒ£ Joint Positions (DOF) Reset

**Khá»Ÿi táº¡o vá»‹ trÃ­ khá»›p**:

```python
# default_dof_pos tá»« config
# + Random noise: Â±0.1 rad
dof_pos[env_ids] = default_dof_pos + torch_rand_float(-0.1, 0.1, shape)
dof_vel[env_ids] = 0.0  # Váº­n tá»‘c khá»›p = 0
```

**VÃ­ dá»¥ (H1 Walking)**:
```python
default_joint_angles = {
    'left_hip_yaw_joint': 0.0,
    'left_hip_pitch_joint': -0.4,  # Squat position
    'left_knee_joint': 0.8,         # Knee bent
    'left_ankle_pitch_joint': -0.4,
    'right_hip_yaw_joint': 0.0,
    'right_hip_pitch_joint': -0.4,
    'right_knee_joint': 0.8,
    'right_ankle_pitch_joint': -0.4,
    'torso_joint': 0.0,
    'left_shoulder_pitch_joint': 0.0,  # Arm neutral
    'right_shoulder_pitch_joint': 0.0,
}
# Random Â±0.1 rad thÃªm vÃ o má»—i joint
```

---

### 2ï¸âƒ£ Root Position (Base) Reset

**Khá»Ÿi táº¡o vá»‹ trÃ­ thÃ¢n chÃ­nh**:

```python
# Base position
if custom_origins:  # CÃ³ terrain curriculum
    root_states[env_ids] = base_init_state
    root_states[env_ids, :3] += env_origins[env_ids]
    # XY random: Â±1m quanh center
    root_states[env_ids, :2] += torch_rand_float(-1., 1., shape)
else:  # Plane terrain
    root_states[env_ids] = base_init_state
    # Center point
    root_states[env_ids, :3] += env_origins[env_ids]

# Base velocity
root_states[env_ids, 7:13] = 0  # KhÃ´ng cÃ³ váº­n tá»‘c ban Ä‘áº§u
# [7:10]: linear velocity (0, 0, 0)
# [10:13]: angular velocity (0, 0, 0)
```

**Default Base Position (config)**:
```python
class init_state:
    pos = [0.0, 0.0, 1.0]  # x, y, z [m]
    # z = 1.0m (chiá»u cao ban Ä‘áº§u Ä‘á»ƒ chÃ¢n khÃ´ng cháº¡m ground)
    rot = [0.0, 0.0, 0.0, 1.0]  # quaternion (neutral)
    lin_vel = [0.0, 0.0, 0.0]
    ang_vel = [0.0, 0.0, 0.0]
```

---

### 3ï¸âƒ£ Command Reset

**Resample má»¥c tiÃªu má»›i**:

```python
# Walking: Random velocity commands
commands[:, 0] = uniform(-1.0, 2.0)  # lin_vel_x
commands[:, 1] = uniform(-1.0, 1.0)  # lin_vel_y
commands[:, 2] = uniform(-1.0, 1.0)  # ang_vel_yaw
commands[:, 3] = uniform(-3.14, 3.14) # heading

# Task manipulation (transfer): Random object positions
commands[:, 0:3] = random_wrist_position()  # 3D wrist target
commands[:, 3:6] = random_box_position()    # 3D box target
commands[:, 6:9] = random_other_params()    # Extra params
```

---

### 4ï¸âƒ£ Buffer Reset

**Clear history**:

```python
last_last_actions[env_ids] = 0.0
actions[env_ids] = 0.0
last_actions[env_ids] = 0.0
last_rigid_state[env_ids] = 0.0
last_dof_vel[env_ids] = 0.0
feet_air_time[env_ids] = 0.0
episode_length_buf[env_ids] = 0  # Counter reset to 0
reset_buf[env_ids] = 1  # Mark as reset
```

---

## ğŸ“Š Timeline VÃ­ Dá»¥ (Walking)

```
Time: 0s - Episode starts
â”œâ”€ reset_idx() Ä‘Æ°á»£c gá»i
â”‚  â”œâ”€ DOF positions: default Â±0.1
â”‚  â”œâ”€ Base position: [0, 0, 1.0] + curriculum offset
â”‚  â”œâ”€ Command: lin_vel_x=0.5, lin_vel_y=0.0, ang_vel_yaw=0.1
â”‚  â””â”€ Buffers cleared
â”‚
â”œâ”€ [0-24s] Episode running
â”‚  â”œâ”€ Step 0-24000: Agent executes actions
â”‚  â”œâ”€ Check termination each step:
â”‚  â”‚  â”œâ”€ Did robot touch forbidden body parts? â†’ YES = terminate
â”‚  â”‚  â”œâ”€ Time > 24s? â†’ YES = terminate
â”‚  â”‚  â””â”€ NO = continue
â”‚  â””â”€ Accumulate rewards
â”‚
â”œâ”€ Time: 24s - Timeout reached
â”‚  â””â”€ reset_idx() â†’ New episode starts
â”‚
OR

â”œâ”€ Time: 5.2s - Robot ngÃ£
â”‚  â”œâ”€ Contact with ['pelvis'] detected
â”‚  â”œâ”€ reset_buf set to 1
â”‚  â””â”€ reset_idx() â†’ New episode starts (5.2s < 24s)
```

---

## ğŸ¯ Key Parameters

| Param | Value | Meaning |
|---|---|---|
| `episode_length_s` | 24s (loco), 8s (task) | Max duration |
| `dt` | 0.001s | Simulation timestep |
| `decimation` | 10 | Policy updates every 10 sim steps |
| `dof_init_noise` | Â±0.1 rad | DOF position randomness |
| `base_init_pos` | [0, 0, 1.0] | ThÃ¢n chÃ­nh ban Ä‘áº§u |
| `termination_contacts` | ['pelvis', 'torso', 'shoulder', 'elbow'] | Forbidden body parts |

---

## ğŸ“Œ Tuning Tips

**Náº¿u episode quÃ¡ ngáº¯n** (robot ngÃ£ nhanh):
- Kiá»ƒm tra `terminate_after_contacts_on` - quÃ¡ nhiá»u body parts?
- TÄƒng `dof_init_noise` Ä‘á»ƒ agent há»c tá»« diverse states

**Náº¿u episode quÃ¡ dÃ i** (khÃ´ng converge):
- Giáº£m `episode_length_s`
- Hoáº·c tÄƒng termination reward (-1.0 penalty)

**Náº¿u robot khÃ´ng reset properly**:
- Kiá»ƒm tra `curriculum` settings
- Kiá»ƒm tra `custom_origins` - terrain offset cÃ³ tÃ­nh?
