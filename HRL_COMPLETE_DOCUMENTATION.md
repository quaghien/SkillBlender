# HRL (Hierarchical Reinforcement Learning) - COMPLETE TECHNICAL DOCUMENTATION

**Version:** v6.0 PPO-Correct + HRL Storage Fix  
**Last Updated:** 2026-01-13  
**File:** `rsl_rl/rsl_rl/modules/actor_critic_hrl_simple.py`

---

## QUICK REFERENCE

| Parameter | Value |
|-----------|-------|
| **Tasks** | 8 (reach, button, cabinet, ball, box, transfer, lift, carry) |
| **Skills** | 4 (walk, reach, squat, step) |
| **Actor Input** | 105D (State 69 + Goal 14 + Mask 14 + TaskID 8) |
| **Critic Input** | 303D (3 frames √ó 101D) |
| **Command Output** | 14D (split into 4 skill slices) |
| **Episode Length** | 1000 steps (20s √ó 50Hz) |
| **Learning Rate** | 1e-5 (fixed, author's value) |
| **Num Envs** | 16384 |
| **Batch Size** | 983,040 (16384 √ó 60) |

---

## 1. LOW-LEVEL SKILL COMMAND SYSTEM

### 1.1 T·ªïng quan

High-level policy output 14D command vector. M·ªói skill ch·ªâ s·ª≠ d·ª•ng m·ªôt **slice** c·ªßa vector n√†y:

```
Command Vector (14D):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [0:3]      ‚îÇ [3:9]           ‚îÇ [9:11]   ‚îÇ [11:14]          ‚îÇ
‚îÇ Walk (3D)  ‚îÇ Reach (6D)      ‚îÇ Squat(2D)‚îÇ Step (3D)        ‚îÇ
‚îÇ vx,vy,œâ    ‚îÇ l_xyz, r_xyz    ‚îÇ h, hold  ‚îÇ feet_xy delta    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Chi ti·∫øt Command c·ªßa t·ª´ng Skill

#### üö∂ SKILL 0: WALKING (3D) - slice [0:3]

| Index | Name | Min | Max | Unit | M√¥ t·∫£ |
|-------|------|-----|-----|------|-------|
| 0 | `vx` | -1.0 | 1.0 | m/s | V·∫≠n t·ªëc t·ªãnh ti·∫øn theo tr·ª•c X (tr∆∞·ªõc/sau) |
| 1 | `vy` | -1.0 | 1.0 | m/s | V·∫≠n t·ªëc t·ªãnh ti·∫øn theo tr·ª•c Y (tr√°i/ph·∫£i) |
| 2 | `omega` | -1.0 | 1.0 | rad/s | V·∫≠n t·ªëc g√≥c quay (xoay quanh tr·ª•c Z) |

**Ngu·ªìn g·ªëc range:** `h1_walking_config.py` lines 167-169
```python
class ranges:
    lin_vel_x = [-1.0, 2.0]  # Config says 2.0 but only stable at [-1, 1]
    lin_vel_y = [-1.0, 1.0]
    ang_vel_yaw = [-1.0, 1.0]
```

**‚ö†Ô∏è L∆ØU √ù:** Config g·ªëc cho ph√©p `vx` t·ªõi 2.0 m/s nh∆∞ng th·ª±c t·∫ø robot **CH·ªà ·ªîN ƒê·ªäNH** trong kho·∫£ng `[-1, 1]` m/s. V∆∞·ª£t qu√° s·∫Ω g√¢y ng√£!

---

#### ü§ö SKILL 1: REACHING (6D) - slice [3:9]

| Index | Name | Min | Max | Unit | M√¥ t·∫£ |
|-------|------|-----|-----|------|-------|
| 3 | `l_wrist_x` | -0.10 | 0.25 | m | Delta X tay tr√°i (so v·ªõi default pos) |
| 4 | `l_wrist_y` | -0.10 | 0.25 | m | Delta Y tay tr√°i |
| 5 | `l_wrist_z` | -0.25 | 0.25 | m | Delta Z tay tr√°i |
| 6 | `r_wrist_x` | -0.10 | 0.25 | m | Delta X tay ph·∫£i |
| 7 | `r_wrist_y` | -0.25 | 0.10 | m | Delta Y tay ph·∫£i (asymmetric!) |
| 8 | `r_wrist_z` | -0.25 | 0.25 | m | Delta Z tay ph·∫£i |

**Ngu·ªìn g·ªëc range:** `h1_reaching_config.py` lines 174-181
```python
class ranges:
    wrist_max_radius = 0.25
    l_wrist_pos_x = [-0.10, 0.25]
    l_wrist_pos_y = [-0.10, 0.25]
    l_wrist_pos_z = [-0.25, 0.25]
    r_wrist_pos_x = [-0.10, 0.25]
    r_wrist_pos_y = [-0.25, 0.10]  # NOTE: Asymmetric!
    r_wrist_pos_z = [-0.25, 0.25]
```

**‚ö†Ô∏è L∆ØU √ù:**
- ƒê√¢y l√† **DELTA position** so v·ªõi default wrist position, KH√îNG ph·∫£i absolute position
- `r_wrist_y` c√≥ range **b·∫•t ƒë·ªëi x·ª©ng** [-0.25, 0.10] do kinematics tay ph·∫£i
- `sample_wp()` trong `utils/human.py` sample points trong sphere r·ªìi filter theo ranges

---

#### üßé SKILL 2: SQUATTING (2D) - slice [9:11]

| Index | Name | Min | Max | Unit | M√¥ t·∫£ |
|-------|------|-----|-----|------|-------|
| 9 | `root_height` | 0.2 | 1.1 | m | Chi·ªÅu cao target c·ªßa root (pelvis) |
| 10 | `placeholder` | 0.0 | 1.0 | - | Kh√¥ng s·ª≠ d·ª•ng (padding) |

**Ngu·ªìn g·ªëc range:** `h1_squatting_config.py` lines 172-176
```python
class ranges:
    root_height_std = 0.2
    min_root_height = 0.2   # Squat th·∫•p nh·∫•t
    max_root_height = 1.1   # ƒê·ª©ng cao nh·∫•t
```

**‚ö†Ô∏è L∆ØU √ù:**
- Default standing height l√† ~0.89m (`base_height_target`)
- `root_height < 0.2m` s·∫Ω g√¢y collapse
- `root_height > 1.1m` kh√¥ng kh·∫£ thi v·ªÅ kinematics

---

#### ü¶∂ SKILL 3: STEPPING (3D) - slice [11:14]

| Index | Name | Min | Max | Unit | M√¥ t·∫£ |
|-------|------|-----|-----|------|-------|
| 11 | `l_foot_dx` | -0.25 | 0.25 | m | Delta X ch√¢n tr√°i |
| 12 | `l_foot_dy` | -0.25 | 0.25 | m | Delta Y ch√¢n tr√°i |
| 13 | `r_foot_dx` | -0.25 | 0.25 | m | Delta X ch√¢n ph·∫£i |

**Ngu·ªìn g·ªëc range:** `h1_stepping_config.py` line 174
```python
class ranges:
    feet_max_radius = 0.25  # Maximum step distance
```

**‚ö†Ô∏è L∆ØU √ù:**
- ƒê√¢y l√† **DELTA position** so v·ªõi current foot position
- `sample_fp()` trong `utils/human.py` sample v·ªõi constraint: m·ªôt ch√¢n ƒë·ª©ng y√™n, ch√¢n kia di chuy·ªÉn
- Radius = 0.25m l√† kho·∫£ng c√°ch b∆∞·ªõc t·ªëi ƒëa an to√†n

---

### 1.3 Command Ranges trong Code

```python
# File: rsl_rl/rsl_rl/modules/actor_critic_hrl_simple.py

SKILL_CMD_SLICES = {
    0: slice(0, 3),    # Walk: [vx, vy, omega]
    1: slice(3, 9),    # Reach: [l_xyz, r_xyz]
    2: slice(9, 11),   # Squat: [height, placeholder]
    3: slice(11, 14),  # Step: [l_xy, r_x]
}

COMMAND_RANGES = {
    0: {  # Walking
        'min': torch.tensor([-1.0, -1.0, -1.0]),
        'max': torch.tensor([1.0, 1.0, 1.0]),
    },
    1: {  # Reaching
        'min': torch.tensor([-0.10, -0.10, -0.25, -0.10, -0.25, -0.25]),
        'max': torch.tensor([0.25, 0.25, 0.25, 0.25, 0.10, 0.25]),
    },
    2: {  # Squatting
        'min': torch.tensor([0.2, 0.0]),
        'max': torch.tensor([1.1, 1.0]),
    },
    3: {  # Stepping
        'min': torch.tensor([-0.25, -0.25, -0.25]),
        'max': torch.tensor([0.25, 0.25, 0.25]),
    },
}
```

---

## 2. COMMAND CLAMPING MECHANISM

### 2.1 T·∫°i sao c·∫ßn Clamp?

Low-level skills ƒë∆∞·ª£c **pre-trained** v·ªõi command ranges c·ª• th·ªÉ. N·∫øu high-level policy generate commands **ngo√†i ranges n√†y**:
- Robot s·∫Ω **ng√£** ho·∫∑c c√≥ h√†nh vi kh√¥ng x√°c ƒë·ªãnh
- Training s·∫Ω **diverge** do unexpected rewards

### 2.2 Implementation

```python
def _clamp_command_to_valid_ranges(self, command_full, skill_exec):
    """
    Clamp command values to valid ranges per skill.
    
    Args:
        command_full: [batch, 14] raw sampled command
        skill_exec: [batch] skill ID being executed
    
    Returns:
        command_clamped: [batch, 14] clamped command
    """
    command_clamped = command_full.clone()
    
    for skill_id in range(self.num_skills):
        mask = (skill_exec == skill_id)
        if not mask.any():
            continue
        
        cmd_slice = self.SKILL_CMD_SLICES[skill_id]
        ranges = self._cmd_ranges_device[skill_id]
        
        # Clamp only the relevant slice
        cmd_vals = command_clamped[mask, cmd_slice]
        cmd_clamped = torch.clamp(cmd_vals, ranges['min'], ranges['max'])
        command_clamped[mask, cmd_slice] = cmd_clamped
    
    return command_clamped
```

### 2.3 Flow trong Forward Pass

```
CommandHead Output (14D mean + std)
         ‚Üì
Sample from Normal(mean, std)  ‚Üí  command_full_raw (14D)
         ‚Üì
_clamp_command_to_valid_ranges()  ‚Üí  command_full (14D, clamped)
         ‚Üì
Extract slice per skill  ‚Üí  command_used (3-6D)
         ‚Üì
Execute low-level skill with command_used
```

### 2.4 Log Prob v·ªõi Clamped Command

**Quan tr·ªçng:** PPO c·∫ßn t√≠nh log_prob c·ªßa **what was SAMPLED**, kh√¥ng ph·∫£i what was executed.

```python
# In forward():
info = {
    'command_full': command_full,         # Clamped - for execution
    'command_full_raw': command_full_raw, # Raw - for log_prob
    ...
}

# In get_actions_log_prob():
command_for_logprob = self.last_info['command_full_raw']  # Use RAW
log_prob_cmd = dist.log_prob(command_for_logprob[slice]).sum()
```

---

## 3. C√ÅCH T√ôY CH·ªàNH COMMAND RANGES

### 3.1 Thay ƒë·ªïi Range cho m·ªôt Skill

**V√≠ d·ª•:** Gi·∫£m walking speed range xu·ªëng [-0.5, 0.5]:

```python
# In actor_critic_hrl_simple.py

COMMAND_RANGES = {
    0: {  # Walking - MODIFIED
        'min': torch.tensor([-0.5, -0.5, -0.5]),  # Slower
        'max': torch.tensor([0.5, 0.5, 0.5]),     # Slower
    },
    # ... other skills unchanged
}
```

### 3.2 Th√™m Skill m·ªõi

**Step 1:** Define slice trong `SKILL_CMD_SLICES`:
```python
SKILL_CMD_SLICES = {
    ...
    4: slice(14, 17),  # New skill: 3D command
}
```

**Step 2:** Define ranges trong `COMMAND_RANGES`:
```python
COMMAND_RANGES = {
    ...
    4: {
        'min': torch.tensor([...]),
        'max': torch.tensor([...]),
    },
}
```

**Step 3:** TƒÉng `command_dim` trong `__init__`:
```python
command_dim=17,  # Was 14, now 17
```

**Step 4:** Update `num_skills`:
```python
num_skills=5,  # Was 4
```

### 3.3 Asymmetric Ranges

M·ªôt s·ªë skills c·∫ßn asymmetric ranges (vd: reaching). C√°ch handle:

```python
# Reaching: right wrist Y range is [-0.25, 0.10] NOT [-0.25, 0.25]
1: {
    'min': torch.tensor([-0.10, -0.10, -0.25, -0.10, -0.25, -0.25]),
    'max': torch.tensor([0.25, 0.25, 0.25, 0.25, 0.10, 0.25]),
    #                                          ‚Üë r_wrist_y max = 0.10
},
```

### 3.4 Dynamic Ranges (Curriculum)

N·∫øu mu·ªën ranges thay ƒë·ªïi theo training progress:

```python
def set_training_step(self, step):
    # Curriculum for walking speed
    if step < 1_000_000_000:  # First 1B steps
        self.COMMAND_RANGES[0]['max'][0] = 0.5  # Max vx = 0.5
    else:
        self.COMMAND_RANGES[0]['max'][0] = 1.0  # Max vx = 1.0
    
    # Re-initialize device tensors
    self._cmd_ranges_device[0] = {
        'min': self.COMMAND_RANGES[0]['min'].to(self.device),
        'max': self.COMMAND_RANGES[0]['max'].to(self.device),
    }
```

---

## 4. POLICY ARCHITECTURE

### 4.1 Network Structure

```
Actor Observation (105D)
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Actor Backbone          ‚îÇ
‚îÇ  Linear(105, 512) + ELU     ‚îÇ
‚îÇ  Linear(512, 256) + ELU     ‚îÇ
‚îÇ  Linear(256, 128) + ELU     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
      Features (128D)
         ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì           ‚Üì             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇGating‚îÇ  ‚îÇ Command  ‚îÇ  ‚îÇ Residual ‚îÇ
‚îÇ(4D)  ‚îÇ  ‚îÇ (14D√ó2)  ‚îÇ  ‚îÇ (19D√ó2)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚Üì           ‚Üì             ‚Üì
Skill ID   Mean+Std      Mean+Std
```

### 4.2 Output Heads

#### GatingHead
```python
class GatingHead(nn.Module):
    def __init__(self, feature_dim=128, num_skills=4):
        self.fc = nn.Linear(feature_dim, num_skills)
    
    def forward(self, features):
        logits = self.fc(features)
        probs = F.softmax(logits, dim=-1)
        return logits, probs  # Categorical distribution
```

#### CommandHead (v·ªõi Log-Std Clamping)
```python
class CommandHead(nn.Module):
    LOG_STD_MIN = -5.0  # std >= 0.0067
    LOG_STD_MAX = 2.0   # std <= 7.39
    
    def __init__(self, feature_dim=128, command_dim=14):
        self.fc_mean = nn.Linear(feature_dim, command_dim)
        self.fc_log_std = nn.Linear(feature_dim, command_dim)
    
    def forward(self, features):
        mean = self.fc_mean(features)
        log_std = torch.clamp(self.fc_log_std(features), 
                              self.LOG_STD_MIN, self.LOG_STD_MAX)
        std = torch.exp(log_std)
        return mean, std
```

#### ResidualHead (init near zero)
```python
class ResidualHead(nn.Module):
    LOG_STD_MIN = -5.0
    LOG_STD_MAX = 0.0   # std <= 1.0 (residual should be small)
    
    def __init__(self, feature_dim=128, action_dim=19):
        self.fc_mean = nn.Linear(feature_dim, action_dim)
        self.fc_log_std = nn.Linear(feature_dim, action_dim)
        
        # Init near zero for curriculum
        nn.init.uniform_(self.fc_mean.weight, -1e-4, 1e-4)
        nn.init.zeros_(self.fc_mean.bias)
        nn.init.constant_(self.fc_log_std.bias, -2.0)  # std ~ 0.135
```

---

## 5. FORWARD PASS (CHI TI·∫æT)

```python
def forward(self, obs):
    batch_size = obs.shape[0]
    is_training = (batch_size == self.num_envs)
    
    # 1. Extract features
    features = self.actor_backbone(obs)
    
    # 2. Gating - sample skill
    gating_logits, gating_probs = self.gating_head(features)
    gating_dist = Categorical(probs=gating_probs)
    
    # 3. Hold time logic (keep skill for 3 steps)
    if is_training:
        sample_mask = self.hold_time.should_sample()
        skill_new = gating_dist.sample()
        skill_exec, is_held = self.hold_time.update(skill_new, sample_mask)
    else:
        skill_exec = gating_dist.sample()
        is_held = torch.zeros(batch_size, dtype=torch.bool, device=self.device)
    
    # 4. Command - sample all 14D
    cmd_mean, cmd_std = self.command_head(features)
    cmd_dist = Normal(cmd_mean, cmd_std)
    command_full_raw = cmd_dist.sample()
    
    # 5. CLAMP command to valid ranges ‚Üê CRITICAL!
    command_full = self._clamp_command_to_valid_ranges(command_full_raw, skill_exec)
    
    # 6. Execute single skill (hard option)
    base_action = self._execute_single_skill(obs, command_full, skill_exec)
    
    # 7. Residual
    res_mean, res_std = self.residual_head(features)
    res_dist = Normal(res_mean, res_std)
    residual_raw = res_dist.sample()
    
    if self.residual_clip > 0:  # Phase 2
        residual = torch.clamp(residual_raw, -self.residual_clip, self.residual_clip)
    else:  # Phase 1 (frozen)
        residual = torch.zeros_like(residual_raw)
    
    # 8. Final action
    action_exec = torch.clamp(base_action + residual, -1.0, 1.0)
    
    # 9. Store info for log_prob
    info = {
        'skill_exec': skill_exec,
        'is_held': is_held,
        'command_full': command_full,
        'command_full_raw': command_full_raw,  # For log_prob!
        'residual_raw': residual_raw,
        'gating_probs': gating_probs,
    }
    
    return action_exec, None, info
```

---

## 6. PPO LOG PROBABILITY

### 6.1 Correct Formula

```python
def get_log_prob(self, skill_exec, command_full_raw, residual_raw, is_held):
    """
    PPO-correct log_prob computation.
    
    Key principles:
    1. Gating: 0 if held (don't penalize past decision)
    2. Command: only dims in used slice
    3. Residual: all 19D
    """
    batch_size = skill_exec.shape[0]
    
    # 1. Gating
    log_prob_gating = self.last_gating_dist.log_prob(skill_exec)
    log_prob_gating = log_prob_gating * (~is_held).float()  # Zero if held
    
    # 2. Command (ONLY used slice!)
    log_prob_cmd = torch.zeros(batch_size, device=self.device)
    for skill_id in range(self.num_skills):
        mask = (skill_exec == skill_id)
        if not mask.any():
            continue
        
        cmd_slice = self.SKILL_CMD_SLICES[skill_id]
        cmd_used = command_full_raw[mask, cmd_slice]  # Use RAW for log_prob
        mean_slice = self.last_command_dist.mean[mask, cmd_slice]
        std_slice = self.last_command_dist.stddev[mask, cmd_slice]
        
        log_prob_slice = Normal(mean_slice, std_slice).log_prob(cmd_used).sum(dim=-1)
        log_prob_cmd[mask] = log_prob_slice
    
    # 3. Residual
    log_prob_res = self.last_residual_dist.log_prob(residual_raw).sum(dim=-1)
    
    total = log_prob_gating + log_prob_cmd + log_prob_res
    return torch.clamp(total, -100, 100)  # Prevent extreme values
```

### 6.2 HRL Storage Fix (CRITICAL)

**V·∫•n ƒë·ªÅ:** PPO update phase g·ªçi `act(obs_batch)` r·ªìi `get_actions_log_prob(actions_batch)`. 
V·ªõi HRL policy, `act()` s·∫Ω **sample M·ªöI** skill/command/residual ‚Üí `last_info` kh√¥ng kh·ªõp v·ªõi stored actions ‚Üí **ratio explode!**

**Gi·∫£i ph√°p:** L∆∞u HRL info v√†o RolloutStorage v√† d√πng `get_actions_log_prob_hrl()` trong update phase.

#### RolloutStorage Changes
```python
# Transition class - th√™m HRL fields
class Transition:
    def __init__(self):
        ...
        self.hrl_skill_exec = None       # [num_envs] int64
        self.hrl_command_raw = None      # [num_envs, 14] float  
        self.hrl_residual_raw = None     # [num_envs, 19] float
        self.hrl_is_held = None          # [num_envs] bool
```

#### PPO Changes
```python
# In act() - save HRL info
if hasattr(self.actor_critic, 'last_info'):
    info = self.actor_critic.last_info
    self.transition.hrl_skill_exec = info['skill_exec'].detach()
    self.transition.hrl_command_raw = info['command_full_raw'].detach()
    self.transition.hrl_residual_raw = info['residual_raw'].detach()
    self.transition.hrl_is_held = info['is_held'].detach()

# In update() - use stored info for log_prob
if hrl_info_batch is not None:
    actions_log_prob_batch = self.actor_critic.get_actions_log_prob_hrl(
        obs_batch,
        hrl_info_batch['skill_exec'],
        hrl_info_batch['command_raw'],
        hrl_info_batch['residual_raw'],
        hrl_info_batch['is_held']
    )
```

#### ActorCriticHRLSimple - New Method
```python
def get_actions_log_prob_hrl(self, obs, skill_exec, command_raw, residual_raw, is_held):
    """
    PPO update phase: Recompute distributions from obs and evaluate log_prob
    for the STORED actions (not newly sampled ones).
    """
    # 1. Recompute distributions from current policy
    features = self.actor_backbone(obs)
    _, gating_probs = self.gating_head(features)
    gating_dist = Categorical(probs=gating_probs)
    cmd_mean, cmd_std = self.command_head(features)
    res_mean, res_std = self.residual_head(features)
    
    # 2. Compute log_prob using STORED actions
    log_prob_gating = gating_dist.log_prob(skill_exec) * (~is_held).float()
    
    log_prob_cmd = torch.zeros(batch_size, device=self.device)
    for skill_id in range(self.num_skills):
        mask = (skill_exec == skill_id)
        if mask.any():
            s = self.SKILL_CMD_SLICES[skill_id]
            log_prob_cmd[mask] = Normal(cmd_mean[mask, s], cmd_std[mask, s]).log_prob(
                command_raw[mask, s]).sum(-1)
    
    log_prob_res = Normal(res_mean, res_std).log_prob(residual_raw).sum(-1)
    
    return torch.clamp(log_prob_gating + log_prob_cmd + log_prob_res, -100, 100)
```

**K·∫øt qu·∫£ sau fix:**
| Metric | Tr∆∞·ªõc (Bug) | Sau (Fixed) |
|--------|-------------|-------------|
| Surrogate loss | 1e6 ~ 1e8 | **~0** ‚úÖ |
| Value loss | 100 ~ 1000 | **29 ~ 800** |
| Ratio | explode | **~1** ‚úÖ |

### 6.3 T·∫°i sao d√πng `command_full_raw`?

| Scenario | What to use |
|----------|-------------|
| **Execute action** | `command_full` (clamped) |
| **Compute log_prob** | `command_full_raw` (sampled) |

PPO c·∫ßn bi·∫øt probability c·ªßa **what was sampled**, kh√¥ng ph·∫£i what was executed.

---

## 7. TRAINING CONFIG

### 7.1 Environment
```python
class H1HRLCfg:
    class env:
        num_envs = 16384
        num_actions = 19
        num_observations = 105
        num_privileged_obs = 303
        episode_length_s = 20
    
    class control:
        decimation = 20  # 50 Hz control
        action_scale = 0.25
```

### 7.2 Algorithm (Author's Values)
```python
class algorithm:
    learning_rate = 1e-5      # Author's value for HRL tasks
    schedule = 'fixed'        # NOT adaptive!
    num_learning_epochs = 2   # Author's value (less epochs)
    num_mini_batches = 4
    clip_param = 0.2
    entropy_coef = 0.001      # Author's value (10x smaller)
    value_loss_coef = 1.0
    max_grad_norm = 1.0
    gamma = 0.994             # Author's value (higher)
    lam = 0.9                 # Author's value (lower)
```

### 7.3 Training Phases
| Phase | Iterations | Residual Clip |
|-------|-----------|---------------|
| 1 | 0 - 49,999 | 0 (frozen) |
| 2 | 50,000+ | ¬±0.05 |

---

## 8. FILES

```
SkillBlender/
‚îú‚îÄ‚îÄ rsl_rl/rsl_rl/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ actor_critic_hrl_simple.py   ‚Üê Main policy (with get_actions_log_prob_hrl)
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ppo.py                       ‚Üê PPO (with HRL info handling)
‚îÇ   ‚îî‚îÄ‚îÄ storage/
‚îÇ       ‚îî‚îÄ‚îÄ rollout_storage.py           ‚Üê Storage (with HRL fields)
‚îÇ
‚îú‚îÄ‚îÄ legged_gym/legged_gym/
‚îÇ   ‚îú‚îÄ‚îÄ envs/h1/h1_hrl/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ h1_hrl.py                    ‚Üê Environment + Config
‚îÇ   ‚îú‚îÄ‚îÄ envs/h1/h1_walking/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ h1_walking_config.py         ‚Üê Walking ranges
‚îÇ   ‚îú‚îÄ‚îÄ envs/h1/h1_reaching/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ h1_reaching_config.py        ‚Üê Reaching ranges
‚îÇ   ‚îú‚îÄ‚îÄ envs/h1/h1_squatting/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ h1_squatting_config.py       ‚Üê Squatting ranges
‚îÇ   ‚îî‚îÄ‚îÄ envs/h1/h1_stepping/
‚îÇ       ‚îî‚îÄ‚îÄ h1_stepping_config.py        ‚Üê Stepping ranges
‚îÇ
‚îî‚îÄ‚îÄ HRL_COMPLETE_DOCUMENTATION.md        ‚Üê This file
```

---

## 9. TROUBLESHOOTING

### Surrogate loss = 1e6 ~ 1e8?
‚Üí **HRL Storage bug!** PPO ƒëang d√πng wrong skill/command/residual.
‚Üí Fix: Ensure `get_actions_log_prob_hrl()` ƒë∆∞·ª£c g·ªçi v·ªõi stored `hrl_info_batch`.

### Robot ng√£ ngay khi training?
‚Üí Command v∆∞·ª£t qu√° valid range. Check clamp logic ƒë√£ ƒë∆∞·ª£c apply ch∆∞a.

### Loss spike to 1e10?
‚Üí Check W&B `Debug/ratio_max`. N·∫øu > 10, log_prob computation c√≥ v·∫•n ƒë·ªÅ.

### Skill kh√¥ng ƒë∆∞·ª£c ch·ªçn ƒë·ªÅu?
‚Üí Check `Debug/gating_probs` ho·∫∑c tƒÉng `entropy_coef`.

### Wrist position kh√¥ng ƒë√∫ng?
‚Üí Reaching command l√† DELTA position, kh√¥ng ph·∫£i absolute. Check offset trong env.

### Episode length qu√° ng·∫Øn (~24)?
‚Üí ƒê√¢y l√† s·ªë **steps**, kh√¥ng ph·∫£i seconds. V·ªõi `num_steps_per_env=60`, robot ƒëang terminate s·ªõm.
‚Üí Check termination conditions trong `h1_hrl.py`.

---

## 10. TRAINING COMMAND

```bash
cd /home/crl/hienhq/SkillBlender/legged_gym

conda activate qhrl && python legged_gym/scripts/train_hrl.py \
    --task h1_hrl \
    --run_name hrl_v6_storage_fix \
    --num_envs 16384 \
    --max_iterations 100000 \
    --sim_device cuda:0 \
    --rl_device cuda:0 \
    --headless \
    --wandb hrl_v6
```

**GPU Memory:** ~12-15 GB with 16384 envs √ó 60 steps (983K batch)

---

## 11. CHANGELOG

| Version | Date | Changes |
|---------|------|--------|
| v6.0 | 2026-01-13 | **HRL Storage Fix** - Store skill/command/residual in RolloutStorage, add `get_actions_log_prob_hrl()` |
| v5.0 | 2026-01-13 | Author's config (LR=1e-5, epochs=2, entropy=0.001, gamma=0.994) |
| v4.2 | 2026-01-13 | Command clamping to valid ranges |
| v4.0 | 2026-01-13 | PPO-Correct log_prob (gating√ócommand√óresidual) |

---

**Document Status:** Complete  
**Code Version:** v6.0 (HRL Storage Fix + Command Clamping + Author's Config)
